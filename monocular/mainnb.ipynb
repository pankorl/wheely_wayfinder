{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "from semSeg import perform_semantic_segmentation\n",
    "from birdsEye import perform_birds_eye_transformation\n",
    "from occGrid import create_occupancy_grid\n",
    "import numpy as np\n",
    "from keras_segmentation.models import all_models\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "import pickle\n",
    "import struct\n",
    "import cv2.aruco as aruco\n",
    "from marker_handler import MarkerHandlers\n",
    "import inspect\n",
    "import zlib\n",
    "\n",
    "stream_camera = False\n",
    "use_aruco = True\n",
    "use_obs_detect = True\n",
    "\n",
    "\n",
    "if use_obs_detect:\n",
    "    model = all_models.model_from_name['vgg_unet'](n_classes=2, input_height=320, input_width=640)\n",
    "    # Create a checkpoint object\n",
    "    checkpoint = tf.train.Checkpoint(model=model)\n",
    "    # Restore from a checkpoint\n",
    "\n",
    "    # ch_path = r'C:\\Users\\simon\\ikt213g23h\\wheely\\monocular\\image-segmentation-keras\\checkpoints\\checkpoints_fixed_numEpochs4.4.index'\n",
    "    # ch_path = r'C:\\Users\\simon\\ikt213g23h\\wheely\\monocular\\image-segmentation-keras\\checkpoints.00005.index'\n",
    "    ch_path = r'C:\\Users\\simon\\ikt213g23h\\wheely\\monocular\\image-segmentation-keras\\checkpoints\\checkpoints_binary_class.4.index'\n",
    "    checkpoint.restore(ch_path).expect_partial()\n",
    "    \n",
    "    # Birds eye view and road crop for obstacle detection\n",
    "    # Points in the source image\n",
    "    pts1 = np.float32([[130, 100], [190, 100], [0, 160], [320, 160]])\n",
    "    pts1 = np.float32([[50, 20], [-50, 20], [0, 160], [320, 160]])\n",
    "    # pts1 = np.float32([[0, 0], [320, 0], [0, 160], [320, 160]])\n",
    "    # Points in the destination image\n",
    "    pts2 = np.float32([[0, 0], [320, 0], [0, 160], [320, 160]])\n",
    "    # Compute the perspective transformation matrix\n",
    "    matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    # Compute the inverse transformation matrix\n",
    "    inverse_matrix = cv2.getPerspectiveTransform(pts2, pts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not stream_camera):\n",
    "    # Initialize webcam\n",
    "    # webcam = cv2.VideoCapture(0)\n",
    "    webcam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    if not webcam.isOpened():\n",
    "        print(\"Could not open webcam\")\n",
    "        exit()\n",
    "\n",
    "else:\n",
    "    # Video Client\n",
    "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client_socket.connect(('192.168.66.85', 8089))\n",
    "    data = b\"\"\n",
    "    payload_size = struct.calcsize(\">L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Class of road pixels\n",
    "class_index = 0\n",
    "\n",
    "if (use_aruco):\n",
    "    # Aruco\n",
    "    # Get the predefined dictionary\n",
    "    arucoDict = aruco.getPredefinedDictionary(aruco.DICT_5X5_50)\n",
    "\n",
    "    detector = aruco.ArucoDetector(arucoDict)\n",
    "\n",
    "    #Load marker handlers:\n",
    "    mh = MarkerHandlers()\n",
    "    handlerFunctions = [func for name, func in inspect.getmembers(mh, inspect.ismethod)]\n",
    "    current_id = 0\n",
    "\n",
    "\n",
    "def capture_frame():\n",
    "    global data\n",
    "    global payload_size\n",
    "    global client_socket\n",
    "    \n",
    "\n",
    "    # Retrieve message size\n",
    "    while len(data) < payload_size:\n",
    "        data += client_socket.recv(4096)\n",
    "\n",
    "    packed_msg_size = data[:payload_size]\n",
    "    data = data[payload_size:]\n",
    "    msg_size = struct.unpack(\">L\", packed_msg_size)[0]\n",
    "\n",
    "    # Retrieve all data based on message size\n",
    "    while len(data) < msg_size:\n",
    "        data += client_socket.recv(4096)\n",
    "\n",
    "    frame_data = data[:msg_size]\n",
    "    data = data[msg_size:]\n",
    "\n",
    "    # Deserialize the frame\n",
    "    frame = pickle.loads(frame_data)\n",
    "    # frame = frame[:160, :320]\n",
    "    return frame\n",
    "\n",
    "\n",
    "def aruco_func(frame):\n",
    "    global current_id\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect the markers in the image using the ArucoDetector object\n",
    "    corners, ids, rejectedImgPoints = detector.detectMarkers(image=gray)\n",
    "\n",
    "    # Check if markers are detected\n",
    "    if ids is not None:\n",
    "        # If markers are detected, overlay their ID and outline them in the frame\n",
    "        aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "    #Checks if new id detected and executes corresponding handler function\n",
    "    if ids is not None:\n",
    "        id = ids[0][0]\n",
    "        if id != current_id:\n",
    "            handlerFunctions[id-1]()  \n",
    "            current_id = id\n",
    "    else:\n",
    "        #Set current_id to 0 current id is removed from frame\n",
    "        current_id = 0\n",
    "\n",
    "\n",
    "def is_obstacle_in_front(frame, orig_frame, obstacle_min_size):\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    crop_frame = thresh[:-20, 30:-30]\n",
    "\n",
    "    contours, _ = cv2.findContours(crop_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    obs_pres = False\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        # Check if the obstacle is in the lower half of the image and meets the minimum size\n",
    "        if y + h >= crop_frame.shape[0] and w * h >= obstacle_min_size:\n",
    "            # rect_pts = np.float32([[x+30, y], [x+30 + w, y], [x+30, y + h], [x+30 + w, y + h]])\n",
    "            rect_pts = np.float32([[x+30 + w, y], [x+30, y], [x+30, y + h], [x+30 + w, y + h]])\n",
    "            transformed_pts = cv2.perspectiveTransform(np.array([rect_pts]), inverse_matrix)[0]\n",
    "            cv2.polylines(orig_frame, [np.int32(2*transformed_pts)], True, (0, 0, 255), 2)\n",
    "            \n",
    "            obstacle_present = True\n",
    "            obs_pres = True\n",
    "\n",
    "    return obs_pres, orig_frame\n",
    "\n",
    "obs_count = 0\n",
    "obs_true_count = 0\n",
    "obs_scan_time = 10\n",
    "obs_threshold = 0.7\n",
    "\n",
    "\n",
    "def inc_obs_count(is_obstacle):\n",
    "    global obs_count\n",
    "    global obs_true_count\n",
    "\n",
    "    if obs_count == 0:\n",
    "        if is_obstacle:\n",
    "            # TODO: Send stop command\n",
    "            print(\"Stop to check if obstacle\")\n",
    "            obs_true_count = 1\n",
    "            obs_count += 1\n",
    "    else:\n",
    "        if is_obstacle:\n",
    "            obs_true_count += 1\n",
    "        obs_count += 1\n",
    "    if obs_count >= obs_scan_time:\n",
    "        if obs_true_count/obs_count >= obs_threshold:\n",
    "            # TODO: Send navigate obstacle command\n",
    "            print(\"Navigate obstacle!!!\")\n",
    "            obs_true_count = 0\n",
    "            obs_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Stop to check if obstacle\n",
      "1/1 [==============================] - 1s 729ms/step\n",
      "1/1 [==============================] - 1s 649ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 773ms/step\n",
      "1/1 [==============================] - 1s 749ms/step\n",
      "1/1 [==============================] - 1s 567ms/step\n",
      "1/1 [==============================] - 0s 499ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "Navigate obstacle!!!\n",
      "1/1 [==============================] - 0s 487ms/step\n",
      "Stop to check if obstacle\n",
      "1/1 [==============================] - 0s 489ms/step\n",
      "1/1 [==============================] - 0s 491ms/step\n",
      "1/1 [==============================] - 0s 490ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 484ms/step\n",
      "1/1 [==============================] - 0s 475ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 1s 610ms/step\n",
      "Navigate obstacle!!!\n",
      "1/1 [==============================] - 1s 687ms/step\n",
      "Stop to check if obstacle\n",
      "1/1 [==============================] - 1s 741ms/step\n",
      "1/1 [==============================] - 1s 728ms/step\n",
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 0s 479ms/step\n",
      "1/1 [==============================] - 0s 476ms/step\n",
      "1/1 [==============================] - 0s 479ms/step\n",
      "1/1 [==============================] - 0s 481ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "\n",
    "    if not stream_camera:\n",
    "        ret, frame = webcam.read()\n",
    "        if not ret:\n",
    "            print('Failed to grab frame')\n",
    "            webcam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "            continue\n",
    "        frame = frame[-320:, -640:]\n",
    "\n",
    "    else:\n",
    "        frame = capture_frame()[-320:, :]\n",
    "\n",
    "    if use_aruco:\n",
    "        aruco_func(frame)\n",
    "\n",
    "\n",
    "    if use_obs_detect:\n",
    "        # Perform semantic segmentation\n",
    "        free_space_map = perform_semantic_segmentation(frame, checkpoint.model, class_index)  \n",
    "        free_space_map = np.reshape(free_space_map, (160, 320))\n",
    "\n",
    "        free_space_map = free_space_map.astype(np.uint8) * 255\n",
    "        free_space_map = cv2.cvtColor(free_space_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        closed = cv2.morphologyEx(free_space_map, cv2.MORPH_CLOSE, kernel)\n",
    "        closed_opened = cv2.morphologyEx(closed, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # cv2.imshow('free_space', free_space_map)\n",
    "        # cv2.imshow('closed freespace', closed)\n",
    "\n",
    "        # print(free_space_map.shape)\n",
    "        # print(np.any(np.isnan(free_space_map)))\n",
    "        # print(np.any(np.isinf(free_space_map)))\n",
    "\n",
    "        # print(\"Model output shape:\", free_space_map.shape)\n",
    "        # print(\"Model output type:\", type(free_space_map))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pts1 = np.float32(pts1)\n",
    "        pts2 = np.float32(pts2)\n",
    "\n",
    "        # Perform bird's-eye view transformation\n",
    "        # bird_eye_view = perform_birds_eye_transformation(free_space_map, pts1, pts2, 320, 160)\n",
    "        bird_eye_view = perform_birds_eye_transformation(closed_opened, pts1, pts2, 320, 160)\n",
    "\n",
    "        cv2.imshow(\"bird_eye\", bird_eye_view)\n",
    "\n",
    "        obs_pres, frame = is_obstacle_in_front(bird_eye_view, frame, 10)\n",
    "        \n",
    "        inc_obs_count(obs_pres)\n",
    "        # if obs_pres:\n",
    "        #     print(\"Obstacle!!\")\n",
    "    \n",
    "    cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "    # Create occupancy grid\n",
    "    # occupancy_grid = create_occupancy_grid(bird_eye_view)\n",
    "\n",
    "    # Display the webcam feed\n",
    "    # cv2.imshow('Webcam Feed', frame)\n",
    "\n",
    "    # Display the occupancy grid\n",
    "    # cv2.imshow('Occupancy Grid', occupancy_grid)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "if stream_camera:\n",
    "    client_socket.close()\n",
    "else:\n",
    "    webcam.release()\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
